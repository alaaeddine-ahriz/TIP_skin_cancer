{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Skin Cancer Detection using MLCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installer des bibliothèques et faire les imports nécessaires"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install tensorflow\n",
    "%pip install matplotlib\n",
    "%pip install numpy\n",
    "%pip install pandas\n",
    "%pip install pydot\n",
    "%pip install pydot graphviz\n",
    "%pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import os\n",
    "from tensorflow.keras import Input\n",
    "from tensorflow.keras.applications import EfficientNetB0 # type: ignore #ingore the warning\n",
    "from tensorflow.keras.layers import Concatenate, Input, Dense, Dropout, Flatten, GlobalAveragePooling2D # type: ignore #ingore the warning\n",
    "from tensorflow.keras.models import Model, Sequential # type: ignore #ingore the warning\n",
    "from tensorflow.keras.optimizers import Adam # type: ignore #ingore the warning\n",
    "from tensorflow.keras.utils import plot_model # type: ignore #ingore the warning\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array # type: ignore #ingore the warning\n",
    "from tensorflow.keras.applications.efficientnet import preprocess_input # type: ignore #ingore the warning\n",
    "from tensorflow.keras.callbacks import TensorBoard # type: ignore #ingore the warning\n",
    "from tensorflow.keras.metrics import Precision, Recall # type: ignore #ingore the warning\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Définition de la fonction de traçage de courbes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_training_curves(training, validation, title, subplot):\n",
    "    \"\"\"\n",
    "    Titre: Afficher les courbes d'entraînement\n",
    "\n",
    "    Description:\n",
    "    Cette fonction trace les courbes d'entraînement et de validation pour une métrique donnée (par exemple, précision, perte) au fil des époques.\n",
    "    Elle configure les sous-graphiques, personnalise l'apparence et ajoute des légendes et des étiquettes pour une meilleure visualisation.\n",
    "\n",
    "    Arguments:\n",
    "        training (liste ou tableau): Les points de données d'entraînement à tracer.\n",
    "        validation (liste ou tableau): Les points de données de validation à tracer.\n",
    "        title (str): Le titre du graphique, généralement le nom de la métrique tracée.\n",
    "        subplot (int): L'index du sous-graphe à utiliser pour le graphique actuel (1 ou 2).\n",
    "\n",
    "    Retour:\n",
    "        Aucun\n",
    "    \"\"\"\n",
    "    if subplot == 1:  # set up the subplots on the first call\n",
    "        plt.subplots(figsize=(10, 10), facecolor='#F0F0F0')\n",
    "        plt.tight_layout()\n",
    "    plt.subplot(2, 1, subplot)  # S'assurer d'avoir un bon placement dans la grille (2x1)\n",
    "    plt.gca().set_facecolor('#F8F8F8')  # change the background color\n",
    "    plt.plot(training)\n",
    "    plt.plot(validation)\n",
    "    plt.title('model ' + title)\n",
    "    plt.ylabel(title)\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'valid.'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Détecter le hardware à disposition (TPU, GPU ou CPU dans cet ordre de priorité)\n",
    "La variable `strategy` détermine le matériel disponible (TPU, GPU ou CPU) et le nombre de répliques synchronisées (`strategy.num_replicas_in_sync`). Cette information sera utilisée pour adapter dynamiquement le `BATCH_SIZE` global, en le multipliant par le nombre de répliques, afin d'optimiser l'entraînement selon les ressources disponibles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detect hardware\n",
    "try:\n",
    "  tpu = tf.distribute.cluster_resolver.TPUClusterResolver() # TPU detection\n",
    "except ValueError:\n",
    "  tpu = None\n",
    "#If TPU not found try with GPUs\n",
    "  gpus = tf.config.experimental.list_logical_devices(\"GPU\")\n",
    "    \n",
    "# Select appropriate distribution strategy for hardware\n",
    "if tpu:\n",
    "  tf.config.experimental_connect_to_cluster(tpu)\n",
    "  tf.tpu.experimental.initialize_tpu_system(tpu)\n",
    "  strategy = tf.distribute.experimental.TPUStrategy(tpu)\n",
    "  print('Running on TPU ', tpu.master())  \n",
    "elif len(gpus) > 0:\n",
    "  strategy = tf.distribute.MirroredStrategy(gpus) # this works for 1 to multiple GPUs\n",
    "  print('Running on ', len(gpus), ' GPU(s) ')\n",
    "else:\n",
    "  strategy = tf.distribute.get_strategy()\n",
    "  print('Running on CPU')\n",
    "\n",
    "# How many accelerators do we have ?\n",
    "print(\"Number of accelerators: \", strategy.num_replicas_in_sync)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Définition des paramètres globaux pour l'entrainement du modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chemins vers les fichiers TFRecord (désactivé dans cet exemple)\n",
    "# TRAINING_FILENAMES = tf.io.gfile.glob(GCS_DS_PATH + '/tfrecords/train*')\n",
    "# TEST_FILENAMES = tf.io.gfile.glob(GCS_DS_PATH + '/tfrecords/test*')\n",
    "\n",
    "# Taille des lots adaptée au nombre de répliques (dispositifs GPU/TPU) disponibles\n",
    "\n",
    "##################################\n",
    "# rajout conidtion si GPU ou TPU #\n",
    "##################################\n",
    "BATCH_SIZE = 32 * strategy.num_replicas_in_sync  # Par exemple, 10 images par réplique\n",
    "\n",
    "# Taille des images à utiliser (100x100 pixels)\n",
    "IMAGE_SIZE = [100, 100]  # Taille utilisée pour le redimensionnement des images\n",
    "imSize = 100             # Taille utilisée pour redimensionner les images dans les pipelines\n",
    "\n",
    "# Optimisation automatique pour le préchargement des données\n",
    "AUTO = tf.data.experimental.AUTOTUNE\n",
    "\n",
    "# Nombre d'époques (itérations sur l'ensemble d'entraînement)\n",
    "EPOCHS = 10  # Peut être ajusté en fonction des performances et de la convergence\n",
    "\n",
    "# Définir l'entrée du modèle (couche d'entrée pour TensorFlow/Keras)\n",
    "input_layer = Input(shape=(imSize, imSize, 3))  # Entrée avec une image RGB (3 canaux)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Traitement des métadonnées, classifications en batch (train, validate, test) et création des dataset TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chargement et prétraitement des métadonnées"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chargement du fichier CSV contenant les métadonnées\n",
    "metadata = pd.read_csv(\"./Data/train.csv\")\n",
    "if metadata.isnull().any().any():\n",
    "    print(\"Des valeurs manquantes ont été détectées.\")\n",
    "    \n",
    "# Ajouter les chemins complets des images\n",
    "metadata['image_path'] = metadata['image_name'].apply(lambda x: f\"./Data/train/{x}.jpg\")\n",
    "\n",
    "metadata['label'] = metadata['benign_malignant'].map({'benign': 0, 'malignant': 1})\n",
    "\n",
    "\n",
    "# Supprimer les lignes avec des valeurs manquantes\n",
    "metadata = metadata.dropna(subset=['label'])\n",
    "\n",
    "print(f\"Nombre total d'images disponibles : {len(metadata)}\")\n",
    "print(metadata['benign_malignant'].isnull().sum())\n",
    "print(metadata['benign_malignant'].unique())\n",
    "print(f\"Total rows in original CSV: {metadata.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Affiche le nombre  d'images par classe\n",
    "print(metadata['label'].value_counts())\n",
    "\n",
    "# Prendre un échantillon aléatoire de 10 000 images\n",
    "metadata = metadata.sample(n=1000, random_state=42)\n",
    "print(metadata['label'].value_counts())\n",
    "# Afficher la taille du nouvel échantillon\n",
    "print(f\"Nombre d'images après échantillonnage : {len(metadata)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Division des données en ensembles d’entraînement, validation et test\n",
    "\n",
    "À cette étape, les données sont divisées en trois ensembles distincts : **entraînement**, **validation**, et **test**, afin de préparer le modèle pour son entraînement et son évaluation. Tout d'abord, 70 % des données sont affectées à l'ensemble d'entraînement, qui sera utilisé pour ajuster les paramètres du modèle. Les 30 % restants sont répartis entre les ensembles de validation (20 % des données totales) et de test (10 % des données totales). La stratification est appliquée sur la variable `label` pour garantir que les proportions des classes (par exemple, \"Malignant\" et \"Benign\") sont similaires dans les trois ensembles. Cette méthode assure une représentation équilibrée des classes dans chaque ensemble, ce qui est essentiel pour éviter les biais dans l'évaluation des performances du modèle.\n",
    "\n",
    "La stratification consiste à répartir les données de manière à ce que la proportion de chaque classe soit conservée dans les sous-ensembles créés (entraînement, validation, test). Cela garantit que les ensembles reflètent fidèlement la distribution initiale des classes, évitant les déséquilibres qui pourraient biaiser l'entraînement ou l'évaluation du modèle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Division des ensembles en entraînement (70%), validation (20%), et test (10%)\n",
    "train_metadata, temp_metadata = train_test_split(\n",
    "    metadata, \n",
    "    test_size=0.3,  # 30 % seront partagés entre validation et test\n",
    "    stratify=metadata['label'],  # Assurer un équilibre des classes\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "val_metadata, test_metadata = train_test_split(\n",
    "    temp_metadata, \n",
    "    test_size=0.33,  # 1/3 des 30 % pour test, soit ~10 % au total\n",
    "    stratify=temp_metadata['label'], \n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Entraînement : {train_metadata.shape[0]} exemples\")\n",
    "print(f\"Validation : {val_metadata.shape[0]} exemples\")\n",
    "print(f\"Test : {test_metadata.shape[0]} exemples\")\n",
    "\n",
    "####################################################################################\n",
    "# ??? Ajouter la fonction de création des datasets pour TensorFlow juste après ??? #\n",
    "####################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vérifications sur les datasets splités."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Class distribution in training set:\")\n",
    "print(train_metadata['label'].value_counts(normalize=True))\n",
    "print(\"Class distribution in validation set:\")\n",
    "print(val_metadata['label'].value_counts(normalize=True))\n",
    "print(\"Class distribution in test set:\")\n",
    "print(test_metadata['label'].value_counts(normalize=True))\n",
    "\n",
    "total_samples = train_metadata.shape[0] + val_metadata.shape[0] + test_metadata.shape[0]\n",
    "print(f\"Total samples: {total_samples} (original: {metadata.shape[0]})\")\n",
    "\n",
    "# Check pour des overlaps entre les sets de données (data leakage), retourne une erreur si overlap\n",
    "assert len(set(train_metadata.index) & set(val_metadata.index)) == 0, \"Overlap between train and validation sets!\"\n",
    "assert len(set(val_metadata.index) & set(test_metadata.index)) == 0, \"Overlap between validation and test sets!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Définition des fonctions pour charger et prétraiter les données\n",
    "À cette étape, le code prépare un pipeline de traitement des images pour l'entraînement du modèle. Les images sont chargées à partir de leurs chemins, redimensionnées à une taille standard de 100x100 pixels, normalisées (valeurs entre 0 et 1), et associées à leurs étiquettes (labels). Ces transformations sont encapsulées dans une fonction `load_image_and_label`, appliquée via `tf.data.Dataset` pour créer un ensemble de données TensorFlow optimisé. Enfin, les données sont divisées en lots et préchargées pour accélérer l'entraînement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction pour charger et prétraiter une image\n",
    "def load_image_and_label(image_path, label):\n",
    "    \"\"\"\n",
    "    Charge une image depuis son chemin, applique des prétraitements\n",
    "    (normalisation, redimensionnement), et retourne l'image et son étiquette.\n",
    "    \"\"\"\n",
    "    # Charger l'image depuis son chemin\n",
    "    image = tf.io.read_file(image_path)\n",
    "    image = tf.image.decode_jpeg(image, channels=3)  # Décode une image JPEG en RGB\n",
    "    \n",
    "    # Normaliser les valeurs des pixels (entre 0 et 1)\n",
    "    image = tf.cast(image, tf.float32) / 255.0\n",
    "    \n",
    "    # Redimensionner l'image à une taille standard\n",
    "    image = tf.image.resize(image, [imSize, imSize])\n",
    "    \n",
    "    return image, label\n",
    "\n",
    "# Fonction pour convertir un DataFrame Pandas en dataset TensorFlow\n",
    "def create_tf_dataset(metadata_df, batch_size):\n",
    "    \"\"\"\n",
    "    Convertit un DataFrame contenant les chemins des images et les labels \n",
    "    en un dataset TensorFlow optimisé pour l'entraînement.\n",
    "    \"\"\"\n",
    "    # Extraction des colonnes nécessaires depuis le DataFrame\n",
    "    image_paths = metadata_df['image_path'].values\n",
    "    labels = metadata_df['label'].values\n",
    "    \n",
    "    # Créer un dataset TensorFlow à partir des chemins et des étiquettes\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((image_paths, labels))\n",
    "    \n",
    "    # Appliquer la fonction de chargement et de prétraitement à chaque image du dataset\n",
    "    dataset = dataset.map(load_image_and_label, num_parallel_calls=AUTO)\n",
    "    \n",
    "    dataset = dataset.repeat()\n",
    "\n",
    "    dataset = dataset.shuffle(1500)  # Mélange avec un tampon de 1500 éléments\n",
    "\n",
    "    # Diviser en lots et activer le préchargement pour optimiser les performances\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    dataset= dataset.prefetch(AUTO)\n",
    "\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test d'affichage de quelques images du dataset (depuis le pipeline TensorFlow)\n",
    "Cette étape permet de vérifier si les images ont été correctement redimensionnées en prenant aléatoirement 5 images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Visualiser 5 images aléatoires depuis le pipeline TensorFlow\n",
    "# for image, label in create_tf_dataset(metadata.sample(frac=1).reset_index(drop=True), batch_size=1).take(5):\n",
    "#     # Convertir le tenseur en tableau NumPy pour l'affichage\n",
    "#     img_resized = image[0].numpy()\n",
    "#     plt.imshow(img_resized)\n",
    "#     # Afficher le label avec le mapping 0 -> Benign, 1 -> Malignant\n",
    "#     plt.title(f\"{'Benign' if label.numpy() == 0 else 'Malignant'}\")\n",
    "#     plt.axis('off')  # Supprimer les axes pour une meilleure visualisation\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Création des datasets TensorFlow\n",
    "\n",
    "Cette étape transforme les sous-ensembles (entraînement, validation et test) en pipelines optimisés pour TensorFlow à l'aide de `create_tf_dataset`. Chaque dataset contient des images prétraitées (chargées, redimensionnées, et normalisées) associées à leurs labels, regroupées en lots de taille spécifiée (`BATCH_SIZE`). Ces datasets sont utilisés directement par le modèle lors de l'entraînement ou de l'évaluation pour assurer une gestion efficace des données."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BATCH_SIZE = max(1, len(train_metadata) // 10)  # Adjust batch size\n",
    "\n",
    "# Créer les datasets\n",
    "training_dataset = create_tf_dataset(train_metadata, batch_size=BATCH_SIZE)\n",
    "validation_dataset = create_tf_dataset(val_metadata, batch_size=BATCH_SIZE)\n",
    "test_dataset = create_tf_dataset(test_metadata, batch_size=BATCH_SIZE)\n",
    "\n",
    "print(f\"Training dataset : {len(training_dataset)} batches\")\n",
    "print(f\"Validation dataset : {len(validation_dataset)} batches\")\n",
    "print(f\"Test dataset : {len(test_dataset)} batches\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calcul des paramètres pour l’entraînement\n",
    "\n",
    "Les paramètres `STEPS_PER_EPOCH` et `VALIDATION_STEPS` déterminent le nombre de lots nécessaires pour parcourir une fois l'ensemble des données d'entraînement ou de validation. Ils sont calculés en divisant la taille totale des données par `BATCH_SIZE` pour garantir que le modèle traite toutes les données à chaque époque, tout en optimisant l'utilisation des ressources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# à supprimer\n",
    "print(f\"Training samples: {len(train_metadata)}\")\n",
    "print(f\"Validation samples: {len(val_metadata)}\")\n",
    "print(f\"Batch size: {BATCH_SIZE}\")\n",
    "\n",
    "# print(f\"Total training batches: {len(list(training_dataset))}\")\n",
    "# print(f\"Total validation batches: {len(list(validation_dataset))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcul des paramètres pour l'entraînement\n",
    "# STEPS_PER_EPOCH = len(training_dataset) // BATCH_SIZE\n",
    "# VALIDATION_STEPS = len(validation_dataset) // BATCH_SIZE \n",
    "steps_per_epochs = len(train_metadata) // BATCH_SIZE\n",
    "validation_steps = len(val_metadata) // BATCH_SIZE\n",
    "\n",
    "print(f\"Nombre d'étapes pour l'entraînement : {steps_per_epochs}\")\n",
    "print(f\"Nombre d'étapes pour la validation : {validation_steps}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Charger le modèle (sans couche de classification finale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Charger le modèle EfficientNetB0 préentraîné sans la couche de classification finale\n",
    "with strategy.scope():\n",
    "    base_model = tf.keras.Sequential([\n",
    "        EfficientNetB0(\n",
    "            input_shape=(imSize, imSize, 3),\n",
    "            weights='imagenet',\n",
    "            include_top=False\n",
    "        )\n",
    "    ])\n",
    "\n",
    "# Geler les couches du modèle de base pour conserver les poids préentraînés\n",
    "base_model.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Passer l'entrée à travers le modèle de base\n",
    "x = base_model(input_layer, training=False)  # `input_layer` \n",
    "x = Flatten()(x)  # Aplatir les caractéristiques extraites\n",
    "\n",
    "# Ajout de couches fully connected pour la classification\n",
    "x = Dense(128, activation=\"relu\")(x)  # Couche dense\n",
    "x = Dropout(0.5)(x)  # Dropout pour régularisation\n",
    "output = Dense(1, activation=\"sigmoid\")(x)  # Sigmoid pour une classification binaire\n",
    "\n",
    "# Définir le modèle final avec une seule entrée (image)\n",
    "model = Model(inputs=input_layer, outputs=output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compiler le modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcul des poids des classes\n",
    "# class_weights = {0: 1.0, 1: 9.0}  # Augmenter le poids de la classe \"malin\"\n",
    "# Ajustez le poids en fonction de l'importance de chaque classe dans votre dataset\n",
    "\n",
    "# Créez une fois les objets Precision et Recall\n",
    "# precision = Precision()\n",
    "# recall = Recall()\n",
    "\n",
    "# def f1_score(y_true, y_pred):\n",
    "#     \"\"\"\n",
    "#     Calcul du F1 score basé sur la précision et le rappel.\n",
    "#     \"\"\"\n",
    "#     p = precision(y_true, y_pred)\n",
    "#     r = recall(y_true, y_pred)\n",
    "#     return 2 * ((p * r) / (p + r + tf.keras.backend.epsilon()))\n",
    "\n",
    "model.compile(\n",
    "    optimizer='adam',  # Optimiseur Adam\n",
    "    loss=\"binary_crossentropy\",          # Fonction de perte pour la classification binaire\n",
    "    metrics=[\"accuracy\"]  # Suivi de la précision\n",
    ")\n",
    "\n",
    "# Résumé du modèle\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation de l'architecture du modèle\n",
    "# plot_model(model, show_shapes=True, to_file=\"efficientnetb0_model.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learning_rate_function(epoch):\n",
    "    LR_START = 0.001 # Taux d'apprentissage initial\n",
    "    LR_MAX = 0.00005 * strategy.num_replicas_in_sync # Taux d'apprentissage maximal\n",
    "    LR_MIN = 0.00001 # Taux d'apprentissage minimal\n",
    "    LR_RAMPUP_EPOCHS = 5 # nombre d'époques pendant lesquelles le taux d'apprentissage augmente linéairement.\n",
    "    LR_SUSTAIN_EPOCHS = 0 # nombre d'époques où le taux reste maximal\n",
    "    LR_EXP_DECAY = .8 # taux de décroissance exponentielle du taux d'apprentissage après les périodes de \"ramp-up\" et de \"soutien\"\n",
    "    \n",
    "\n",
    "    # Augmentation (pour les premières LR_RAMPUP_EPOCHS époques) : le taux d'apprentissage commence à LR_START et monte linéairement jusqu'à LR_MAX\n",
    "    if epoch < LR_RAMPUP_EPOCHS: \n",
    "        lr = (LR_MAX - LR_START) / LR_RAMPUP_EPOCHS * epoch + LR_START\n",
    "\n",
    "    # Soutien (pour les LR_SUSTAIN_EPOCHS suivantes) : le taux d'apprentissage reste constant à LR_MAX\n",
    "    elif epoch < LR_RAMPUP_EPOCHS + LR_SUSTAIN_EPOCHS:\n",
    "        lr = LR_MAX\n",
    "    # Décroissance (pour les époques restantes) : le taux d'apprentissage diminue exponentiellement\n",
    "    else:\n",
    "        lr = (LR_MAX - LR_MIN) * LR_EXP_DECAY**(epoch - LR_RAMPUP_EPOCHS - LR_SUSTAIN_EPOCHS) + LR_MIN\n",
    "    return lr\n",
    "\n",
    "lr_schedule = tf.keras.callbacks.LearningRateScheduler(learning_rate_function, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = os.path.join(\"logs\", \"fit\", \"model_name\")\n",
    "tensorboard_callback = TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "history = model.fit(training_dataset, steps_per_epoch=steps_per_epochs, epochs=EPOCHS,\n",
    "                    validation_data=validation_dataset, validation_steps=validation_steps,callbacks=[lr_schedule,tensorboard_callback],)\n",
    "\n",
    "# tensorboard --logdir=logs/fit pour lancer TensorBoard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Affichage de l'évolution de \"accuracy\" et \"loss\" en fonction des époques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot accuracy\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.title('Model Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Plot loss\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.plot(history.history['loss'], label='Train Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Model Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction pour charger les matrices Y, Cb, Cr à partir du fichier\n",
    "def load_ycbcr_from_file(file_path):\n",
    "    \"\"\"\n",
    "    Charger les matrices Y, Cb, Cr à partir d'un fichier texte.\n",
    "    Les fichiers doivent contenir trois sections avec des matrices pour chaque composante.\n",
    "    \"\"\"\n",
    "    with open(file_path, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    # Identifier les sections et extraire les matrices\n",
    "    y_start = lines.index(\"Matrice de luminance (Y):\\n\") + 1\n",
    "    cb_start = lines.index(\"Matrice de chrominance (Cb):\\n\") + 1\n",
    "    cr_start = lines.index(\"Matrice de chrominance (Cr):\\n\") + 1\n",
    "\n",
    "    y = np.loadtxt(lines[y_start:cb_start - 1], dtype=np.float32)\n",
    "    cb = np.loadtxt(lines[cb_start:cr_start - 1], dtype=np.float32)\n",
    "    cr = np.loadtxt(lines[cr_start:], dtype=np.float32)\n",
    "\n",
    "    return y, cb, cr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction pour préparer l'entrée pour le modèle\n",
    "def prepare_ycbcr_input(y, cb, cr):\n",
    "    \"\"\"\n",
    "    Prépare l'entrée pour le modèle à partir des matrices Y, Cb, Cr.\n",
    "    \"\"\"\n",
    "    # Empiler les canaux Y, Cb, Cr pour former une image (100, 100, 3)\n",
    "    image = np.stack([y, cb, cr], axis=-1)  # Shape: (100, 100, 3)\n",
    "\n",
    "    # Normaliser l'image (en utilisant une normalisation simple, pas celle d'ImageNet)\n",
    "    image = image / 255.0  # Normalisation simple entre 0 et 1\n",
    "\n",
    "    # Ajouter la dimension du batch (1, 100, 100, 3)\n",
    "    image = np.expand_dims(image, axis=0)\n",
    "\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction de prédiction\n",
    "def predict_skin_cancer(file_path):\n",
    "    \"\"\"\n",
    "    Prédire à partir des matrices Y, Cb, Cr chargées depuis un fichier.\n",
    "    \"\"\"\n",
    "    # Charger les matrices Y, Cb, Cr à partir du fichier\n",
    "    y, cb, cr = load_ycbcr_from_file(file_path)\n",
    "\n",
    "    # Préparer l'entrée pour le modèle\n",
    "    ycbcr_input = prepare_ycbcr_input(y, cb, cr)\n",
    "\n",
    "\n",
    "    # Prédiction\n",
    "    prediction = model.predict(ycbcr_input)\n",
    "\n",
    "    # Interprétation\n",
    "    if prediction[0] > 0.5:\n",
    "        return f\"Maligne (probabilité {prediction[0][0]:.2f})\"\n",
    "    else:\n",
    "        return f\"Bénigne (probabilité {1 - prediction[0][0]:.2f})\"\n",
    "\n",
    "# Exemple d'utilisation\n",
    "file_path = \"./DataBase/Matrices/ISIC_0001120_matrices.txt\"\n",
    "result = predict_skin_cancer(file_path)\n",
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
